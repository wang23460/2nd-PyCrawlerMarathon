{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加速：多線程爬蟲\n",
    "\n",
    "\n",
    "\n",
    "* 了解知乎 API 使用方式與回傳內容\n",
    "* 撰寫程式存取 API 且添加標頭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業目標\n",
    "\n",
    "* 找一個之前實作過的爬蟲改用多線程改寫，比較前後時間的差異。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import threading\n",
    "import json\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTT_URL = 'https://www.ptt.cc/bbs/Gossiping/index.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 爬取文章\n",
    "def crawl_article(url):\n",
    "    response = requests.get(url, cookies={'over18': '1'})\n",
    "    \n",
    "    # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
    "    if response.status_code != 200:\n",
    "        print('Error - {} is not available to access'.format(url))\n",
    "        return\n",
    "    \n",
    "    # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    \n",
    "    # 取得文章內容主體\n",
    "    main_content = soup.find(id='main-content')\n",
    "    \n",
    "    # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "    metas = main_content.select('div.article-metaline')\n",
    "    author = ''\n",
    "    title = ''\n",
    "    date = ''\n",
    "    if metas:\n",
    "        if metas[0].select('span.article-meta-value')[0]:\n",
    "            author = metas[0].select('span.article-meta-value')[0].string # author\n",
    "        if metas[1].select('span.article-meta-value')[0]:\n",
    "            title = metas[1].select('span.article-meta-value')[0].string # title\n",
    "        if metas[2].select('span.article-meta-value')[0]:\n",
    "            date = metas[2].select('span.article-meta-value')[0].string # date\n",
    "\n",
    "        # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
    "        #\n",
    "        # .extract() 方法可以參考官方文件\n",
    "        #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
    "        for m in metas:\n",
    "            m.extract()\n",
    "        for m in main_content.select('div.article-metaline-right'):\n",
    "            m.extract()\n",
    "    \n",
    "    # 取得留言區主體\n",
    "    pushes = main_content.find_all('div', class_='push')\n",
    "    for p in pushes:\n",
    "        p.extract()\n",
    "    \n",
    "    # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "    # 透過 regular expression 取得 IP\n",
    "    # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "    try:\n",
    "        ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "        ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "    except Exception as e:\n",
    "        ip = ''\n",
    "    \n",
    "    # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "    # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "    #\n",
    "    # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
    "    #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
    "    filtered = []\n",
    "    for v in main_content.stripped_strings:\n",
    "        # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "        if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "            filtered.append(v)\n",
    "\n",
    "    # 定義一些特殊符號與全形符號的過濾器\n",
    "    expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "    for i in range(len(filtered)):\n",
    "        filtered[i] = re.sub(expr, '', filtered[i])\n",
    "    \n",
    "    # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "    filtered = [i for i in filtered if i]\n",
    "    content = ' '.join(filtered)\n",
    "    \n",
    "    # 處理留言區\n",
    "    # p 計算推文數量\n",
    "    # b 計算噓文數量\n",
    "    # n 計算箭頭數量\n",
    "    p, b, n = 0, 0, 0\n",
    "    messages = []\n",
    "    for push in pushes:\n",
    "        # 假如留言段落沒有 push-tag 就跳過\n",
    "        if not push.find('span', 'push-tag'):\n",
    "            continue\n",
    "        \n",
    "        # 過濾額外空白與換行符號\n",
    "        # push_tag 判斷是推文, 箭頭還是噓文\n",
    "        # push_userid 判斷留言的人是誰\n",
    "        # push_content 判斷留言內容\n",
    "        # push_ipdatetime 判斷留言日期時間\n",
    "        push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "        push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "        push_content = push.find('span', 'push-content').strings\n",
    "        push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "        push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\n",
    "        # 整理打包留言的資訊, 並統計推噓文數量\n",
    "        messages.append({\n",
    "            'push_tag': push_tag,\n",
    "            'push_userid': push_userid,\n",
    "            'push_content': push_content,\n",
    "            'push_ipdatetime': push_ipdatetime})\n",
    "        if push_tag == u'推':\n",
    "            p += 1\n",
    "        elif push_tag == u'噓':\n",
    "            b += 1\n",
    "        else:\n",
    "            n += 1\n",
    "    \n",
    "    # 統計推噓文\n",
    "    # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "    # all 為總共留言數量 \n",
    "    message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
    "    \n",
    "    # 整理文章資訊\n",
    "    data = {\n",
    "        'url': url,\n",
    "        'article_author': author,\n",
    "        'article_title': title,\n",
    "        'article_date': date,\n",
    "        'article_content': content,\n",
    "        'ip': ip,\n",
    "        'message_count': message_count,\n",
    "        'messages': messages\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse Re: [爆卦] 中國網軍在蒐集台灣身分證 - https://www.ptt.cc/bbs/Gossiping/M.1587092228.A.D55.html\n",
      "Parse Re: [問卦] 政治系大一就教台獨嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1587092326.A.D84.html\n",
      "Parse Re: [政治] 華航護照更名公決案今立院闖關　民進黨 - https://www.ptt.cc/bbs/Gossiping/M.1587092359.A.E4E.html\n",
      "Parse [問卦] 年薪不到百萬的都是什麼職業 - https://www.ptt.cc/bbs/Gossiping/M.1587092366.A.38D.html\n",
      "Parse [問卦] FF7ac在當年(2005)算是黑科技動畫嗎 - https://www.ptt.cc/bbs/Gossiping/M.1587092444.A.9A2.html\n",
      "Parse [政治] 受困莫三比克生命受威脅 自稱台灣男子上 - https://www.ptt.cc/bbs/Gossiping/M.1587092449.A.E77.html\n",
      "Parse Re: [新聞] 10多年來堅持不喝牛奶！台灣糖尿病之父 - https://www.ptt.cc/bbs/Gossiping/M.1587092454.A.0F8.html\n",
      "Parse [問卦] 台積電是在噴幾點的啦！ - https://www.ptt.cc/bbs/Gossiping/M.1587092497.A.3B8.html\n",
      "Parse [爆卦] 垃圾三立新聞造假移植圖片又一例!! - https://www.ptt.cc/bbs/Gossiping/M.1587092499.A.00C.html\n",
      "Parse Re: [問卦] 國中基測是不是比較能看出讀書天賦？ - https://www.ptt.cc/bbs/Gossiping/M.1587092523.A.D5C.html\n",
      "Parse Re: [問卦] 你們通通給我尊重林奕含!! - https://www.ptt.cc/bbs/Gossiping/M.1587092527.A.838.html\n",
      "Parse Re: [新聞] 小五女生被狗追趕狂奔猝死 飼主過失致死 - https://www.ptt.cc/bbs/Gossiping/M.1587092530.A.149.html\n",
      "Parse [問卦] 這個月的房貸扣款是不是變少了？ - https://www.ptt.cc/bbs/Gossiping/M.1587092552.A.98D.html\n",
      "Parse [問卦] 如果在外面吃飯中了會算自己倒楣嗎 - https://www.ptt.cc/bbs/Gossiping/M.1587092572.A.F92.html\n",
      "Parse Re: [問卦] 為什麼不一口氣收服魔神仔啊？ - https://www.ptt.cc/bbs/Gossiping/M.1587092602.A.986.html\n",
      "Parse [問卦] 欸幹 通姦罪bug那麼明顯為什麼不修？ - https://www.ptt.cc/bbs/Gossiping/M.1587092611.A.ED4.html\n",
      "Parse Re: [問卦] 歐巴馬的健保 是有多爛???? - https://www.ptt.cc/bbs/Gossiping/M.1587092657.A.7C9.html\n",
      "Parse [問卦] 牙醫系從大一就教植牙嗎 - https://www.ptt.cc/bbs/Gossiping/M.1587092660.A.4CB.html\n",
      "Parse Re: [問卦] 油價破新低狗肉車主哭倒在路邊？ - https://www.ptt.cc/bbs/Gossiping/M.1587092689.A.2B5.html\n",
      "Parse [問卦] 怎樣才能開一家觀光客在吃的店？ - https://www.ptt.cc/bbs/Gossiping/M.1587092707.A.6EF.html\n",
      "Reach the last article\n",
      "共用時： 1.385655403137207\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 對文章列表送出請求並取得列表主體\n",
    "resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "soup = BeautifulSoup(resp.text)\n",
    "main_list = soup.find('div', class_='bbs-screen')\n",
    "all_data = []\n",
    "\n",
    "stime = time.time()\n",
    "# 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "for div in main_list.findChildren('div', recursive=False):\n",
    "    class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "    \n",
    "    # 遇到分隔線要處理的情況\n",
    "    if class_name and 'r-list-sep' in class_name:\n",
    "        print('Reach the last article')\n",
    "        break\n",
    "    \n",
    "    # 遇到目標文章\n",
    "    if class_name and 'r-ent' in class_name:\n",
    "        div_title = div.find('div', class_='title')\n",
    "        a_title = div_title.find('a', href=True)\n",
    "        if a_title:\n",
    "            article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "        else:\n",
    "            article_URL = None\n",
    "            a_title = '<a>本文已刪除</a>'\n",
    "        article_title = a_title.text\n",
    "        print('Parse {} - {}'.format(article_title, article_URL))\n",
    "        \n",
    "        # 呼叫上面寫好的 function 來對文章進行爬蟲\n",
    "        if article_URL:\n",
    "            parse_data = crawl_article(article_URL) # 返回單一文章資訊的字典\n",
    "        \n",
    "        # 將爬完的資料儲存\n",
    "        all_data.append(parse_data)\n",
    "        \n",
    "etime = time.time()\n",
    "print('共用時：',etime-stime )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://www.ptt.cc/bbs/Gossiping/M.1587092707.A.6EF.html',\n",
       " 'article_author': 'palindromes ()',\n",
       " 'article_title': '[問卦] 怎樣才能開一家觀光客在吃的店？',\n",
       " 'article_date': 'Fri Apr 17 11:05:05 2020',\n",
       " 'article_content': '我發現一堆觀光客的店\\n\\n明明很難吃卻能大排長龍\\n\\n詭異程度就如八卦某些爆文\\n\\n沒質還能推到爆這樣\\n\\n\\n奇怪的是\\n\\n本地人不愛\\n\\n外地人吃了一次就不去\\n\\n但總是人排 有人買\\n\\n這就是觀光客在吃的店厲害的地方\\n\\n那麼問題來了\\n\\n要想成為一家觀光客在吃的店\\n\\n到底要怎麼做呢？\\n\\n\\n有沒有八卦？\\n\\n\\n\\n\\n\\n\\n\\n\\nhttps://www.ptt.cc/bbs/Gossiping/M.1587092707.A.6EF.html',\n",
       " 'ip': '111.83.36.14',\n",
       " 'message_count': {'all': 0, 'count': 0, 'push': 0, 'boo': 0, 'neutral': 0},\n",
       " 'messages': []}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **多線程爬蟲**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse [新聞] 已婚女醫健身愛上大肌肌,瞞醫師夫偷生娃. - https://www.ptt.cc/bbs/Gossiping/M.1587092710.A.176.html\n",
      "Parse [爆卦] <Nature>公開向汙名化武漢肺炎公開道歉!! - https://www.ptt.cc/bbs/Gossiping/M.1587092722.A.0F6.html\n",
      "Parse Re: [新聞] 超車自撞牽拖別人竟連孕婦都打！9屁孩國 - https://www.ptt.cc/bbs/Gossiping/M.1587092734.A.E59.html\n",
      "Parse [問卦] 打算騎Gogoro去司馬庫斯要怎麼計劃 - https://www.ptt.cc/bbs/Gossiping/M.1587092749.A.1F5.html\n",
      "Parse [問卦] 藥局有強制加入健保藥局嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1587092782.A.C0D.html\n",
      "Parse [新聞] 女兒載閨密遭酒駕撞　母路口舉牌求援… - https://www.ptt.cc/bbs/Gossiping/M.1587092862.A.0B3.html\n",
      "Parse Re: [問卦] 歐巴馬的健保 是有多爛???? - https://www.ptt.cc/bbs/Gossiping/M.1587092876.A.3F6.html\n",
      "Reach the last article\n",
      "共用時： 0.12828803062438965\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 對文章列表送出請求並取得列表主體\n",
    "resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "soup = BeautifulSoup(resp.text)\n",
    "main_list = soup.find('div', class_='bbs-screen')\n",
    "all_data = []\n",
    "all_url = []\n",
    "\n",
    "stime = time.time()\n",
    "# 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "for div in main_list.findChildren('div', recursive=False):\n",
    "    class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "    \n",
    "    # 遇到分隔線要處理的情況\n",
    "    if class_name and 'r-list-sep' in class_name:\n",
    "        print('Reach the last article')\n",
    "        break\n",
    "    \n",
    "    # 遇到目標文章\n",
    "    if class_name and 'r-ent' in class_name:\n",
    "        div_title = div.find('div', class_='title')\n",
    "        a_title = div_title.find('a', href=True)\n",
    "        if a_title:\n",
    "            article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "        else:\n",
    "            article_URL = None\n",
    "            a_title = '<a>本文已刪除</a>'\n",
    "        article_title = a_title.text\n",
    "        print('Parse {} - {}'.format(article_title, article_URL))\n",
    "        \n",
    "        # 把文章連結存在list\n",
    "        if article_URL:\n",
    "            all_url.append(article_URL)\n",
    "\n",
    "# 從這裡丟給子執行緒工作            \n",
    "# 建立 n 個子執行緒，分別去抓文章內容\n",
    "threads = []\n",
    "for i in range(len(all_url)):\n",
    "    threads.append(threading.Thread(target = crawl_article, args = (all_url[i],)))\n",
    "    threads[i].start()\n",
    "\n",
    "# 主執行緒繼續執行自己的工作\n",
    "# ...\n",
    "\n",
    "# 等待所有子執行緒結束\n",
    "for i in range(len(all_url)):\n",
    "    threads[i].join()\n",
    "\n",
    "        \n",
    "etime = time.time()\n",
    "print('共用時：',etime-stime )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTask(threading.Thread):\n",
    "    def __init__(self, task_name):\n",
    "        super(MyTask, self).__init__()\n",
    "        self.task_name = task_name\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Get task: {}\\n\".format(self.task_name))\n",
    "        time.sleep(1)\n",
    "        print(\"Finish task: {}\\n\".format(self.task_name))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = [1,2,3,4,5,6,7,8,9,10]\n",
    "    tasks = []\n",
    "    for i in range(0, 10):\n",
    "        # 建立 task\n",
    "        tasks.append(MyTask(\"task_{}\".format(data[i])))\n",
    "    for t in tasks:\n",
    "        # 開始執行 task\n",
    "        t.start()\n",
    "\n",
    "    for t in tasks:\n",
    "        # 等待 task 執行完畢\n",
    "        # 完畢前會阻塞住主執行緒\n",
    "        t.join()\n",
    "    print(\"Finish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse [新聞] 已婚女醫健身愛上大肌肌,瞞醫師夫偷生娃. - https://www.ptt.cc/bbs/Gossiping/M.1587092710.A.176.html\n",
      "Parse [爆卦] <Nature>公開向汙名化武漢肺炎公開道歉!! - https://www.ptt.cc/bbs/Gossiping/M.1587092722.A.0F6.html\n",
      "Parse Re: [新聞] 超車自撞牽拖別人竟連孕婦都打！9屁孩國 - https://www.ptt.cc/bbs/Gossiping/M.1587092734.A.E59.html\n",
      "Parse [問卦] 打算騎Gogoro去司馬庫斯要怎麼計劃 - https://www.ptt.cc/bbs/Gossiping/M.1587092749.A.1F5.html\n",
      "Parse [問卦] 藥局有強制加入健保藥局嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1587092782.A.C0D.html\n",
      "Parse [新聞] 女兒載閨密遭酒駕撞　母路口舉牌求援… - https://www.ptt.cc/bbs/Gossiping/M.1587092862.A.0B3.html\n",
      "Parse Re: [問卦] 歐巴馬的健保 是有多爛???? - https://www.ptt.cc/bbs/Gossiping/M.1587092876.A.3F6.html\n",
      "Parse Re: [政治] 華航護照更名公決案今立院闖關　民進黨 - https://www.ptt.cc/bbs/Gossiping/M.1587092893.A.245.html\n",
      "Parse Re: [問卦] 今天請假 大台北 哪裡可以逛 ? - https://www.ptt.cc/bbs/Gossiping/M.1587092937.A.B30.html\n",
      "Reach the last article\n",
      "共9個連結\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092710.A.176.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092722.A.0F6.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092734.A.E59.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092749.A.1F5.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092782.A.C0D.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092862.A.0B3.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092876.A.3F6.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092893.A.245.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092937.A.B30.html\n",
      "\n",
      "共用時： 0.1547994613647461\n"
     ]
    }
   ],
   "source": [
    "PTT_URL = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "\n",
    "class Crawl_Article(threading.Thread):\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        super(Crawl_Article, self).__init__()\n",
    "        self.url = url\n",
    "\n",
    "    # 原crawl_article，改成子執行緒run任務\n",
    "    def run(self): \n",
    "        print(\"Get子執行緒: {}\\n\".format(self.url))\n",
    "\n",
    "        response = requests.get(self.url, cookies={'over18': '1'})\n",
    "\n",
    "        # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
    "        if response.status_code != 200:\n",
    "            print('Error - {} is not available to access'.format(self.url))\n",
    "            return\n",
    "\n",
    "        # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
    "        soup = BeautifulSoup(response.text)\n",
    "\n",
    "        # 取得文章內容主體\n",
    "        main_content = soup.find(id='main-content')\n",
    "\n",
    "        # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "        metas = main_content.select('div.article-metaline') #list\n",
    "        author = ''\n",
    "        title = ''\n",
    "        date = ''\n",
    "        if metas:\n",
    "            if metas[0].select('span.article-meta-value')[0]:\n",
    "                author = metas[0].select('span.article-meta-value')[0].string\n",
    "            if metas[1].select('span.article-meta-value')[0]:\n",
    "                title = metas[1].select('span.article-meta-value')[0].string\n",
    "            if metas[2].select('span.article-meta-value')[0]:\n",
    "                date = metas[2].select('span.article-meta-value')[0].string\n",
    "\n",
    "            # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
    "            #\n",
    "            # .extract() 方法可以參考官方文件\n",
    "            #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
    "            for m in metas:\n",
    "                m.extract()\n",
    "            for m in main_content.select('div.article-metaline-right'):\n",
    "                m.extract()\n",
    "\n",
    "        # 取得留言區主體\n",
    "        pushes = main_content.find_all('div', class_='push')\n",
    "        for p in pushes:\n",
    "            p.extract()\n",
    "\n",
    "        # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "        # 透過 regular expression 取得 IP\n",
    "        # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "        try:\n",
    "            ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "            ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "        except Exception as e:\n",
    "            ip = ''\n",
    "\n",
    "        # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "        # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "        #\n",
    "        # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
    "        #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
    "        filtered = []\n",
    "        for v in main_content.stripped_strings:\n",
    "            # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "            if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "                filtered.append(v)\n",
    "\n",
    "        # 定義一些特殊符號與全形符號的過濾器\n",
    "        expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "        for i in range(len(filtered)):\n",
    "            filtered[i] = re.sub(expr, '', filtered[i])\n",
    "\n",
    "        # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "        filtered = [i for i in filtered if i]\n",
    "        content = ' '.join(filtered)\n",
    "\n",
    "        # 處理留言區\n",
    "        # p 計算推文數量\n",
    "        # b 計算噓文數量\n",
    "        # n 計算箭頭數量\n",
    "        p, b, n = 0, 0, 0\n",
    "        messages = []\n",
    "        for push in pushes:\n",
    "            # 假如留言段落沒有 push-tag 就跳過\n",
    "            if not push.find('span', 'push-tag'):\n",
    "                continue\n",
    "\n",
    "            # 過濾額外空白與換行符號\n",
    "            # push_tag 判斷是推文, 箭頭還是噓文\n",
    "            # push_userid 判斷留言的人是誰\n",
    "            # push_content 判斷留言內容\n",
    "            # push_ipdatetime 判斷留言日期時間\n",
    "            push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "            push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "            push_content = push.find('span', 'push-content').strings\n",
    "            push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "            push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\n",
    "            # 整理打包留言的資訊, 並統計推噓文數量\n",
    "            messages.append({\n",
    "                'push_tag': push_tag,\n",
    "                'push_userid': push_userid,\n",
    "                'push_content': push_content,\n",
    "                'push_ipdatetime': push_ipdatetime})\n",
    "            if push_tag == u'推':\n",
    "                p += 1\n",
    "            elif push_tag == u'噓':\n",
    "                b += 1\n",
    "            else:\n",
    "                n += 1\n",
    "\n",
    "        # 統計推噓文\n",
    "        # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "        # all 為總共留言數量 \n",
    "        message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
    "\n",
    "        # 整理文章資訊\n",
    "        data = {\n",
    "            'url': self.url,\n",
    "            'article_author': author,\n",
    "            'article_title': title,\n",
    "            'article_date': date,\n",
    "            'article_content': content,\n",
    "            'ip': ip,\n",
    "            'message_count': message_count,\n",
    "            'messages': messages\n",
    "        }\n",
    "        return data\n",
    "\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 對文章列表送出請求並取得列表主體\n",
    "    resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "    main_list = soup.find('div', class_='bbs-screen')\n",
    "    all_data = []\n",
    "    all_url = []\n",
    "\n",
    "    stime = time.time()\n",
    "    # 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "    for div in main_list.findChildren('div', recursive=False):\n",
    "        class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "        # 遇到分隔線要處理的情況\n",
    "        if class_name and 'r-list-sep' in class_name:\n",
    "            print('Reach the last article')\n",
    "            break\n",
    "        # 遇到目標文章\n",
    "        if class_name and 'r-ent' in class_name:\n",
    "            div_title = div.find('div', class_='title')\n",
    "            a_title = div_title.find('a', href=True)\n",
    "            if a_title:\n",
    "                article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "            else:\n",
    "                article_URL = None\n",
    "                a_title = '<a>本文已刪除</a>'\n",
    "            article_title = a_title.text\n",
    "            print('Parse {} - {}'.format(article_title, article_URL))\n",
    "            # 把文章連結存在list\n",
    "            if article_URL:\n",
    "                all_url.append(article_URL)\n",
    "    \n",
    "    print('共{}個連結'.format(len(all_url)))\n",
    "    # 從這裡丟給子執行緒工作            \n",
    "    # 建立 n 個子執行緒，分別去抓文章內容\n",
    "    threads = []\n",
    "    for i in range(len(all_url)):\n",
    "        threads.append(Crawl_Article(all_url[i]))\n",
    "        threads[i].start()\n",
    "\n",
    "    # 主執行緒繼續執行自己的工作\n",
    "    # ...\n",
    "\n",
    "    # 等待所有子執行緒結束\n",
    "    for i in range(len(all_url)):\n",
    "        threads[i].join()\n",
    "\n",
    "\n",
    "    etime = time.time()\n",
    "    print('共用時：',etime-stime )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **使用佇列 Queue**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse [新聞] 已婚女醫健身愛上大肌肌,瞞醫師夫偷生娃. - https://www.ptt.cc/bbs/Gossiping/M.1587092710.A.176.html\n",
      "Parse [爆卦] <Nature>公開向汙名化武漢肺炎公開道歉!! - https://www.ptt.cc/bbs/Gossiping/M.1587092722.A.0F6.html\n",
      "Parse Re: [新聞] 超車自撞牽拖別人竟連孕婦都打！9屁孩國 - https://www.ptt.cc/bbs/Gossiping/M.1587092734.A.E59.html\n",
      "Parse [問卦] 打算騎Gogoro去司馬庫斯要怎麼計劃 - https://www.ptt.cc/bbs/Gossiping/M.1587092749.A.1F5.html\n",
      "Parse [問卦] 藥局有強制加入健保藥局嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1587092782.A.C0D.html\n",
      "Parse [新聞] 女兒載閨密遭酒駕撞　母路口舉牌求援… - https://www.ptt.cc/bbs/Gossiping/M.1587092862.A.0B3.html\n",
      "Parse Re: [問卦] 歐巴馬的健保 是有多爛???? - https://www.ptt.cc/bbs/Gossiping/M.1587092876.A.3F6.html\n",
      "Parse Re: [政治] 華航護照更名公決案今立院闖關　民進黨 - https://www.ptt.cc/bbs/Gossiping/M.1587092893.A.245.html\n",
      "Parse Re: [問卦] 今天請假 大台北 哪裡可以逛 ? - https://www.ptt.cc/bbs/Gossiping/M.1587092937.A.B30.html\n",
      "Parse [問卦] 有沒有商家拿 0 確診做促銷的八卦 - https://www.ptt.cc/bbs/Gossiping/M.1587092946.A.64D.html\n",
      "Parse Re: [問卦] 美國航母不過是東風21D的靶船不是嗎?? - https://www.ptt.cc/bbs/Gossiping/M.1587092986.A.B8A.html\n",
      "Reach the last article\n",
      "共11個連結\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092710.A.176.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092722.A.0F6.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092734.A.E59.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092749.A.1F5.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092782.A.C0D.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092862.A.0B3.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092876.A.3F6.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092893.A.245.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092937.A.B30.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092946.A.64D.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1587092986.A.B8A.html\n",
      "\n",
      "共用時： 0.1768805980682373\n"
     ]
    }
   ],
   "source": [
    "from queue import Queue\n",
    "\n",
    "PTT_URL = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "\n",
    "class Crawl_Article(threading.Thread):\n",
    "    \n",
    "    def __init__(self, queue):\n",
    "        super(Crawl_Article, self).__init__()\n",
    "        self.queue = queue\n",
    "\n",
    "    # 原crawl_article，改成子執行緒run任務\n",
    "    def run(self): \n",
    "        # 當 queue 裡面有資料再執行\n",
    "        while self.queue.qsize() > 0:\n",
    "            url = self.queue.get()\n",
    "            print(\"Get子執行緒: {}\\n\".format(url))\n",
    "\n",
    "            response = requests.get(url, cookies={'over18': '1'})\n",
    "\n",
    "            # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
    "            if response.status_code != 200:\n",
    "                print('Error - {} is not available to access'.format(url))\n",
    "                return\n",
    "\n",
    "            # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
    "            soup = BeautifulSoup(response.text)\n",
    "\n",
    "            # 取得文章內容主體\n",
    "            main_content = soup.find(id='main-content')\n",
    "\n",
    "            # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "            metas = main_content.select('div.article-metaline') #list\n",
    "            author = ''\n",
    "            title = ''\n",
    "            date = ''\n",
    "            if metas:\n",
    "                if metas[0].select('span.article-meta-value')[0]:\n",
    "                    author = metas[0].select('span.article-meta-value')[0].string\n",
    "                if metas[1].select('span.article-meta-value')[0]:\n",
    "                    title = metas[1].select('span.article-meta-value')[0].string\n",
    "                if metas[2].select('span.article-meta-value')[0]:\n",
    "                    date = metas[2].select('span.article-meta-value')[0].string\n",
    "\n",
    "                # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
    "                #\n",
    "                # .extract() 方法可以參考官方文件\n",
    "                #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
    "                for m in metas:\n",
    "                    m.extract()\n",
    "                for m in main_content.select('div.article-metaline-right'):\n",
    "                    m.extract()\n",
    "\n",
    "            # 取得留言區主體\n",
    "            pushes = main_content.find_all('div', class_='push')\n",
    "            for p in pushes:\n",
    "                p.extract()\n",
    "\n",
    "            # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "            # 透過 regular expression 取得 IP\n",
    "            # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "            try:\n",
    "                ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "                ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "            except Exception as e:\n",
    "                ip = ''\n",
    "\n",
    "            # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "            # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "            #\n",
    "            # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
    "            #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
    "            filtered = []\n",
    "            for v in main_content.stripped_strings:\n",
    "                # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "                if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "                    filtered.append(v)\n",
    "\n",
    "            # 定義一些特殊符號與全形符號的過濾器\n",
    "            expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "            for i in range(len(filtered)):\n",
    "                filtered[i] = re.sub(expr, '', filtered[i])\n",
    "\n",
    "            # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "            filtered = [i for i in filtered if i]\n",
    "            content = ' '.join(filtered)\n",
    "\n",
    "            # 處理留言區\n",
    "            # p 計算推文數量\n",
    "            # b 計算噓文數量\n",
    "            # n 計算箭頭數量\n",
    "            p, b, n = 0, 0, 0\n",
    "            messages = []\n",
    "            for push in pushes:\n",
    "                # 假如留言段落沒有 push-tag 就跳過\n",
    "                if not push.find('span', 'push-tag'):\n",
    "                    continue\n",
    "\n",
    "                # 過濾額外空白與換行符號\n",
    "                # push_tag 判斷是推文, 箭頭還是噓文\n",
    "                # push_userid 判斷留言的人是誰\n",
    "                # push_content 判斷留言內容\n",
    "                # push_ipdatetime 判斷留言日期時間\n",
    "                push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "                push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "                push_content = push.find('span', 'push-content').strings\n",
    "                push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "                push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\n",
    "                # 整理打包留言的資訊, 並統計推噓文數量\n",
    "                messages.append({\n",
    "                    'push_tag': push_tag,\n",
    "                    'push_userid': push_userid,\n",
    "                    'push_content': push_content,\n",
    "                    'push_ipdatetime': push_ipdatetime})\n",
    "                if push_tag == u'推':\n",
    "                    p += 1\n",
    "                elif push_tag == u'噓':\n",
    "                    b += 1\n",
    "                else:\n",
    "                    n += 1\n",
    "\n",
    "            # 統計推噓文\n",
    "            # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "            # all 為總共留言數量 \n",
    "            message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
    "\n",
    "            # 整理文章資訊\n",
    "            data = {\n",
    "                'url': url,\n",
    "                'article_author': author,\n",
    "                'article_title': title,\n",
    "                'article_date': date,\n",
    "                'article_content': content,\n",
    "                'ip': ip,\n",
    "                'message_count': message_count,\n",
    "                'messages': messages\n",
    "            }\n",
    "            return data\n",
    "\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 對文章列表送出請求並取得列表主體\n",
    "    resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "    main_list = soup.find('div', class_='bbs-screen')\n",
    "    all_data = []\n",
    "    \n",
    "    Q_url = Queue()\n",
    "\n",
    "    stime = time.time()\n",
    "    # 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "    for div in main_list.findChildren('div', recursive=False):\n",
    "        class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "        # 遇到分隔線要處理的情況\n",
    "        if class_name and 'r-list-sep' in class_name:\n",
    "            print('Reach the last article')\n",
    "            break\n",
    "        # 遇到目標文章\n",
    "        if class_name and 'r-ent' in class_name:\n",
    "            div_title = div.find('div', class_='title')\n",
    "            a_title = div_title.find('a', href=True)\n",
    "            if a_title:\n",
    "                article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "            else:\n",
    "                article_URL = None\n",
    "                a_title = '<a>本文已刪除</a>'\n",
    "            article_title = a_title.text\n",
    "            print('Parse {} - {}'.format(article_title, article_URL))\n",
    "            # 把文章連結存在list\n",
    "            if article_URL:\n",
    "                Q_url.put(article_URL)\n",
    "    \n",
    "    print('共{}個連結'.format(Q_url.qsize()))\n",
    "    \n",
    "    # 從這裡丟給子執行緒工作            \n",
    "    # 建立 n 個子執行緒，分別去抓文章內容\n",
    "    threads = []\n",
    "    for i in range(Q_url.qsize()):\n",
    "        threads.append(Crawl_Article(Q_url))\n",
    "        threads[i].start()\n",
    "\n",
    "    # 主執行緒繼續執行自己的工作\n",
    "    # ...\n",
    "\n",
    "    # 等待所有子執行緒結束\n",
    "    for i in range(len(all_url)):\n",
    "        threads[i].join()\n",
    "\n",
    "\n",
    "    etime = time.time()\n",
    "    print('共用時：',etime-stime )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **使用lock:**\n",
    "* 被 Lock 的 acquire 與 release 包起來的這段程式碼不會被兩個執行緒同時執行。\n",
    "* 用來寫入檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "子執行緒 100 取得lock\n",
      "子執行緒 100: 寫入檔案 Url 1\n",
      "子執行緒 100 釋放lock\n",
      "子執行緒 100 取得lock\n",
      "子執行緒 100: 寫入檔案 Url 3\n",
      "子執行緒 100 釋放lock\n",
      "子執行緒 100 取得lock\n",
      "子執行緒 100: 寫入檔案 Url 4\n",
      "子執行緒 100 釋放lock\n",
      "子執行緒 100 取得lock\n",
      "子執行緒 100: 寫入檔案 Url 5\n",
      "子執行緒 100 釋放lock\n",
      "子執行緒 200 取得lock\n",
      "子執行緒 200: 寫入檔案 Url 2\n",
      "子執行緒 200 釋放lock\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "class Worker(threading.Thread):\n",
    "    \n",
    "    def __init__(self, queue, num, lock):\n",
    "        \n",
    "        threading.Thread.__init__(self)\n",
    "        self.queue = queue\n",
    "        self.num = num\n",
    "        self.lock = lock\n",
    "\n",
    "    def run(self):\n",
    "        while self.queue.qsize() > 0:\n",
    "            url = self.queue.get()\n",
    "\n",
    "            # 取得 lock\n",
    "            lock.acquire()\n",
    "            print(\"子執行緒 %d 取得lock\" % self.num)\n",
    "\n",
    "            # 不能讓多個執行緒同時進的工作\n",
    "            print(\"子執行緒 %d: 寫入檔案 %s\" % (self.num, url))\n",
    "            time.sleep(1)\n",
    "\n",
    "            # 釋放 lock\n",
    "            print(\"子執行緒 %d 釋放lock\" % self.num)\n",
    "            self.lock.release()\n",
    "            \n",
    "#建立一個佇列\n",
    "my_queue = Queue()\n",
    "\n",
    "#假裝放五個URL進去queue\n",
    "for i in range(1,6):\n",
    "    my_queue.put(\"Url %d\" % i)\n",
    "\n",
    "# 建立 lock\n",
    "lock = threading.Lock()\n",
    "\n",
    "#建立2個子執行緒，傳入queue和一個參數和lock\n",
    "my_worker1 = Worker(my_queue, 100, lock)\n",
    "my_worker2 = Worker(my_queue, 200, lock)\n",
    "\n",
    "my_worker1.start()\n",
    "my_worker2.start()\n",
    "\n",
    "my_worker1.join()\n",
    "my_worker2.join()\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse Re: [新聞] 已婚女醫健身愛上大肌肌,瞞醫師夫偷生娃. - https://www.ptt.cc/bbs/Gossiping/M.1587093717.A.E9A.html\n",
      "Parse Re: [政治] 華航護照更名公決案今立院闖關　民進黨 - https://www.ptt.cc/bbs/Gossiping/M.1587093787.A.B5D.html\n",
      "Parse [問卦] 湘北打山王每看必哭嗎? - https://www.ptt.cc/bbs/Gossiping/M.1587093789.A.143.html\n",
      "Parse [爆卦] 中國官方訂正武漢市確診病例數和死亡數 - https://www.ptt.cc/bbs/Gossiping/M.1587093796.A.836.html\n",
      "Parse Re: [新聞] 小五女生被狗追趕狂奔猝死 飼主過失致死 - https://www.ptt.cc/bbs/Gossiping/M.1587093821.A.2AC.html\n",
      "Parse [政治] 「司馬昭之心全民皆知！」柯建銘批國民黨 - https://www.ptt.cc/bbs/Gossiping/M.1587093822.A.04F.html\n",
      "Parse Re: [政治] 華航護照更名公決案今立院闖關　民進黨 - https://www.ptt.cc/bbs/Gossiping/M.1587093831.A.606.html\n",
      "Parse Re: [新聞] 1萬8罰款未繳房子被法拍 婦人跪求無法挽回 - https://www.ptt.cc/bbs/Gossiping/M.1587093885.A.491.html\n",
      "Parse Re: [爆卦] <Nature>公開向汙名化武漢肺炎公開道歉!! - https://www.ptt.cc/bbs/Gossiping/M.1587093951.A.AB5.html\n",
      "Parse Re: [問卦] 法律系從大一就教廢死嗎 - https://www.ptt.cc/bbs/Gossiping/M.1587093960.A.A0F.html\n",
      "Parse [政治] 鄭麗君夫婦存款少3800多萬 仍超過1億元 - https://www.ptt.cc/bbs/Gossiping/M.1587093986.A.41F.html\n",
      "Parse Re: [新聞] 女兒載閨密遭酒駕撞　母路口舉牌求援… - https://www.ptt.cc/bbs/Gossiping/M.1587094012.A.5A6.html\n",
      "Parse [問卦] 大家認為歐美解封條件是什麼？ - https://www.ptt.cc/bbs/Gossiping/M.1587094015.A.C56.html\n",
      "Parse Re: [新聞] 已婚女醫健身愛上大肌肌,瞞醫師夫偷生娃. - https://www.ptt.cc/bbs/Gossiping/M.1587094033.A.BB6.html\n",
      "Parse Re: [問卦] 新竹的生活有這麼廢嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1587094051.A.AD6.html\n",
      "Parse [問卦] 公衛系從大一就開始教蒸口罩嗎? - https://www.ptt.cc/bbs/Gossiping/M.1587094065.A.E0A.html\n",
      "Parse [問卦] 有沒有渣男名單 - https://www.ptt.cc/bbs/Gossiping/M.1587094069.A.5EF.html\n",
      "Parse Re: [新聞] 中國加強管制出口防疫物資卡關 美政府跳 - https://www.ptt.cc/bbs/Gossiping/M.1587094115.A.DFC.html\n",
      "Parse Re: [問卦] 歐巴馬的健保 是有多爛???? - https://www.ptt.cc/bbs/Gossiping/M.1587094148.A.700.html\n",
      "Parse [問卦] 大媽敢買10張兆豐金不敢買一張台G的八掛? - https://www.ptt.cc/bbs/Gossiping/M.1587094153.A.581.html\n",
      "Reach the last article\n",
      "共20個連結\n",
      "共用時： 0.03863024711608887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-49:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\Anaconda3\\lib\\threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<ipython-input-24-31d037cd2a71>\", line 149, in run\n",
      "    with open('../Data/PTT_Article.json', 'a+', encoding='utf-8') as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../Data/PTT_Article.json'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from queue import Queue\n",
    "\n",
    "PTT_URL = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "\n",
    "class Crawl_Article(threading.Thread):\n",
    "    \n",
    "    def __init__(self, queue, lock):\n",
    "        super(Crawl_Article, self).__init__()\n",
    "        self.queue = queue\n",
    "        self.lock = lock\n",
    "\n",
    "    # 原crawl_article，改成子執行緒run任務\n",
    "    def run(self): \n",
    "        # 當 queue 裡面有資料再執行\n",
    "        while self.queue.qsize() > 0:\n",
    "            url = self.queue.get()\n",
    "            #print(\"Get子執行緒: {}\\n\".format(url))\n",
    "\n",
    "            response = requests.get(url, cookies={'over18': '1'})\n",
    "\n",
    "            # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
    "            if response.status_code != 200:\n",
    "                print('Error - {} is not available to access'.format(url))\n",
    "                return\n",
    "\n",
    "            # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
    "            soup = BeautifulSoup(response.text)\n",
    "\n",
    "            # 取得文章內容主體\n",
    "            main_content = soup.find(id='main-content')\n",
    "\n",
    "            # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "            metas = main_content.select('div.article-metaline') #list\n",
    "            author = ''\n",
    "            title = ''\n",
    "            date = ''\n",
    "            if metas:\n",
    "                if metas[0].select('span.article-meta-value')[0]:\n",
    "                    author = metas[0].select('span.article-meta-value')[0].string\n",
    "                if metas[1].select('span.article-meta-value')[0]:\n",
    "                    title = metas[1].select('span.article-meta-value')[0].string\n",
    "                if metas[2].select('span.article-meta-value')[0]:\n",
    "                    date = metas[2].select('span.article-meta-value')[0].string\n",
    "\n",
    "                # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
    "                #\n",
    "                # .extract() 方法可以參考官方文件\n",
    "                #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
    "                for m in metas:\n",
    "                    m.extract()\n",
    "                for m in main_content.select('div.article-metaline-right'):\n",
    "                    m.extract()\n",
    "\n",
    "            # 取得留言區主體\n",
    "            pushes = main_content.find_all('div', class_='push')\n",
    "            for p in pushes:\n",
    "                p.extract()\n",
    "\n",
    "            # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "            # 透過 regular expression 取得 IP\n",
    "            # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "            try:\n",
    "                ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "                ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "            except Exception as e:\n",
    "                ip = ''\n",
    "\n",
    "            # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "            # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "            #\n",
    "            # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
    "            #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
    "            filtered = []\n",
    "            for v in main_content.stripped_strings:\n",
    "                # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "                if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "                    filtered.append(v)\n",
    "\n",
    "            # 定義一些特殊符號與全形符號的過濾器\n",
    "            expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "            for i in range(len(filtered)):\n",
    "                filtered[i] = re.sub(expr, '', filtered[i])\n",
    "\n",
    "            # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "            filtered = [i for i in filtered if i]\n",
    "            content = ' '.join(filtered)\n",
    "\n",
    "            # 處理留言區\n",
    "            # p 計算推文數量\n",
    "            # b 計算噓文數量\n",
    "            # n 計算箭頭數量\n",
    "            p, b, n = 0, 0, 0\n",
    "            messages = []\n",
    "            for push in pushes:\n",
    "                # 假如留言段落沒有 push-tag 就跳過\n",
    "                if not push.find('span', 'push-tag'):\n",
    "                    continue\n",
    "\n",
    "                # 過濾額外空白與換行符號\n",
    "                # push_tag 判斷是推文, 箭頭還是噓文\n",
    "                # push_userid 判斷留言的人是誰\n",
    "                # push_content 判斷留言內容\n",
    "                # push_ipdatetime 判斷留言日期時間\n",
    "                push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "                push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "                push_content = push.find('span', 'push-content').strings\n",
    "                push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "                push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\n",
    "                # 整理打包留言的資訊, 並統計推噓文數量\n",
    "                messages.append({\n",
    "                    'push_tag': push_tag,\n",
    "                    'push_userid': push_userid,\n",
    "                    'push_content': push_content,\n",
    "                    'push_ipdatetime': push_ipdatetime})\n",
    "                if push_tag == u'推':\n",
    "                    p += 1\n",
    "                elif push_tag == u'噓':\n",
    "                    b += 1\n",
    "                else:\n",
    "                    n += 1\n",
    "\n",
    "            # 統計推噓文\n",
    "            # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "            # all 為總共留言數量 \n",
    "            message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
    "\n",
    "            # 整理文章資訊\n",
    "            data = {\n",
    "                'url': url,\n",
    "                'article_author': author,\n",
    "                'article_title': title,\n",
    "                'article_date': date,\n",
    "                'article_content': content,\n",
    "                'ip': ip,\n",
    "                'message_count': message_count,\n",
    "                'messages': messages\n",
    "            }\n",
    "            \n",
    "            \n",
    "            # 寫入檔案:單一文章內容\n",
    "            \n",
    "            # 取得 lock\n",
    "            lock.acquire()\n",
    "            #print(\"%s 取得lock\" % url[32:51])\n",
    "\n",
    "            # 不能讓多個執行緒同時進的工作 : 將爬完的資訊存成 json 檔案\n",
    "            #print(\"寫入檔案\")\n",
    "            with open('../Data/PTT_Article.json', 'a+', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "                f.write(\",\")\n",
    "\n",
    "            # 釋放 lock\n",
    "            #print(\"%s 釋放lock\" % url[32:51])\n",
    "            self.lock.release()\n",
    "\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 對文章列表送出請求並取得列表主體\n",
    "    resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "    main_list = soup.find('div', class_='bbs-screen')\n",
    "    all_data = []\n",
    "    \n",
    "    Q_url = Queue()\n",
    "\n",
    "    stime = time.time()\n",
    "    # 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "    for div in main_list.findChildren('div', recursive=False):\n",
    "        class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "        # 遇到分隔線要處理的情況\n",
    "        if class_name and 'r-list-sep' in class_name:\n",
    "            print('Reach the last article')\n",
    "            break\n",
    "        # 遇到目標文章\n",
    "        if class_name and 'r-ent' in class_name:\n",
    "            div_title = div.find('div', class_='title')\n",
    "            a_title = div_title.find('a', href=True)\n",
    "            \n",
    "            if a_title:\n",
    "                article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "                article_title = a_title.text\n",
    "            else:\n",
    "                article_URL = None\n",
    "                a_title = '<a>本文已刪除</a>'\n",
    "                article_title = a_title\n",
    "                \n",
    "            #article_title = a_title.text\n",
    "            print('Parse {} - {}'.format(article_title, article_URL))\n",
    "            # 把文章連結存在list\n",
    "            if article_URL:\n",
    "                Q_url.put(article_URL)\n",
    "    \n",
    "    print('共{}個連結'.format(Q_url.qsize()))\n",
    "    \n",
    "    # 建立 lock\n",
    "    lock = threading.Lock()\n",
    "    \n",
    "    # 從這裡丟給子執行緒工作            \n",
    "    # 建立 n 個子執行緒，分別去抓文章內容\n",
    "    threads = []\n",
    "    for i in range(Q_url.qsize()):\n",
    "        threads.append(Crawl_Article(Q_url, lock))\n",
    "        threads[i].start()\n",
    "\n",
    "    # 主執行緒繼續執行自己的工作\n",
    "    # ...\n",
    "\n",
    "    # 等待所有子執行緒結束\n",
    "    # for i in range(len(all_url)):\n",
    "    #     threads[i].join()\n",
    "\n",
    "\n",
    "    etime = time.time()\n",
    "    print('共用時：',etime-stime )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "homework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
